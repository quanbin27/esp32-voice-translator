{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "import pathlib\n",
    "import random\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow.data as tf_data\n",
    "import tensorflow.strings as tf_strings\n",
    "\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras import ops\n",
    "from keras.layers import TextVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.ops as ops\n",
    "\n",
    "\n",
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(dense_dim, activation=\"relu\"),\n",
    "                layers.Dense(embed_dim),\n",
    "            ]\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        if mask is not None:\n",
    "            padding_mask = ops.cast(mask[:, None, :], dtype=\"int32\")\n",
    "        else:\n",
    "            padding_mask = None\n",
    "\n",
    "        attention_output = self.attention(\n",
    "            query=inputs, value=inputs, key=inputs, attention_mask=padding_mask\n",
    "        )\n",
    "        proj_input = self.layernorm_1(inputs + attention_output)\n",
    "        proj_output = self.dense_proj(proj_input)\n",
    "        return self.layernorm_2(proj_input + proj_output)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update(\n",
    "            {\n",
    "                \"embed_dim\": self.embed_dim,\n",
    "                \"dense_dim\": self.dense_dim,\n",
    "                \"num_heads\": self.num_heads,\n",
    "            }\n",
    "        )\n",
    "        return config\n",
    "\n",
    "\n",
    "class PositionalEmbedding(layers.Layer):\n",
    "    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.token_embeddings = layers.Embedding(\n",
    "            input_dim=vocab_size, output_dim=embed_dim\n",
    "        )\n",
    "        self.position_embeddings = layers.Embedding(\n",
    "            input_dim=sequence_length, output_dim=embed_dim\n",
    "        )\n",
    "        self.sequence_length = sequence_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "    def call(self, inputs):\n",
    "        length = ops.shape(inputs)[-1]\n",
    "        positions = ops.arange(0, length, 1)\n",
    "        embedded_tokens = self.token_embeddings(inputs)\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        return embedded_tokens + embedded_positions\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return ops.not_equal(inputs, 0)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update(\n",
    "            {\n",
    "                \"sequence_length\": self.sequence_length,\n",
    "                \"vocab_size\": self.vocab_size,\n",
    "                \"embed_dim\": self.embed_dim,\n",
    "            }\n",
    "        )\n",
    "        return config\n",
    "\n",
    "\n",
    "class TransformerDecoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, latent_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention_1 = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.attention_2 = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(latent_dim, activation=\"relu\"),\n",
    "                layers.Dense(embed_dim),\n",
    "            ]\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "        self.layernorm_3 = layers.LayerNormalization()\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        inputs, encoder_outputs = inputs\n",
    "        causal_mask = self.get_causal_attention_mask(inputs)\n",
    "\n",
    "        if mask is None:\n",
    "            inputs_padding_mask, encoder_outputs_padding_mask = None, None\n",
    "        else:\n",
    "            inputs_padding_mask, encoder_outputs_padding_mask = mask\n",
    "\n",
    "        attention_output_1 = self.attention_1(\n",
    "            query=inputs,\n",
    "            value=inputs,\n",
    "            key=inputs,\n",
    "            attention_mask=causal_mask,\n",
    "            query_mask=inputs_padding_mask,\n",
    "        )\n",
    "        out_1 = self.layernorm_1(inputs + attention_output_1)\n",
    "\n",
    "        attention_output_2 = self.attention_2(\n",
    "            query=out_1,\n",
    "            value=encoder_outputs,\n",
    "            key=encoder_outputs,\n",
    "            query_mask=inputs_padding_mask,\n",
    "            key_mask=encoder_outputs_padding_mask,\n",
    "        )\n",
    "        out_2 = self.layernorm_2(out_1 + attention_output_2)\n",
    "\n",
    "        proj_output = self.dense_proj(out_2)\n",
    "        return self.layernorm_3(out_2 + proj_output)\n",
    "\n",
    "    def get_causal_attention_mask(self, inputs):\n",
    "        input_shape = ops.shape(inputs)\n",
    "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
    "        i = ops.arange(sequence_length)[:, None]\n",
    "        j = ops.arange(sequence_length)\n",
    "        mask = ops.cast(i >= j, dtype=\"int32\")\n",
    "        mask = ops.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
    "        mult = ops.concatenate(\n",
    "            [ops.expand_dims(batch_size, -1), ops.convert_to_tensor([1, 1])],\n",
    "            axis=0,\n",
    "        )\n",
    "        return ops.tile(mask, mult)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update(\n",
    "            {\n",
    "                \"embed_dim\": self.embed_dim,\n",
    "                \"latent_dim\": self.latent_dim,\n",
    "                \"num_heads\": self.num_heads,\n",
    "            }\n",
    "        )\n",
    "        return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.saving import register_keras_serializable\n",
    "\n",
    "\n",
    "\n",
    "vocab_size = 50000\n",
    "sequence_length = 30\n",
    "batch_size = 64\n",
    "\n",
    "strip_chars = string.punctuation + \"Â¿\"\n",
    "strip_chars = strip_chars.replace(\"[\", \"\")\n",
    "strip_chars = strip_chars.replace(\"]\", \"\")\n",
    "\n",
    "\n",
    "\n",
    "@keras.saving.register_keras_serializable()\n",
    "def custom_standardization(input_string):\n",
    "    lowercase = tf_strings.lower(input_string)\n",
    "    return tf_strings.regex_replace(lowercase, \"[%s]\" % re.escape(strip_chars), \"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python_3.12\\Lib\\site-packages\\keras\\src\\layers\\layer.py:393: UserWarning: `build()` was called on layer 'positional_embedding', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n",
      "d:\\Python_3.12\\Lib\\site-packages\\keras\\src\\layers\\layer.py:393: UserWarning: `build()` was called on layer 'positional_embedding_1', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n",
      "d:\\Python_3.12\\Lib\\site-packages\\keras\\src\\layers\\layer.py:393: UserWarning: `build()` was called on layer 'transformer_encoder', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n",
      "d:\\Python_3.12\\Lib\\site-packages\\keras\\src\\layers\\layer.py:393: UserWarning: `build()` was called on layer 'transformer_decoder', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\Python_3.12\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:216: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loaded_transformer = keras.models.load_model(r\"D:\\Ardunio_Ide\\Nhung\\Full\\transformer_model.keras\", custom_objects={\n",
    "    \"PositionalEmbedding\": PositionalEmbedding,\n",
    "    \"TransformerEncoder\": TransformerEncoder,\n",
    "    \"TransformerDecoder\": TransformerDecoder,\n",
    "})\n",
    "\n",
    "\n",
    "\n",
    "import pickle\n",
    "from keras.layers import TextVectorization\n",
    "\n",
    "# Äá»c tá»« file\n",
    "with open(r\"D:\\Ardunio_Ide\\Nhung\\Full\\eng_vocab.pkl\", \"rb\") as f:\n",
    "    eng_vocab = pickle.load(f)\n",
    "\n",
    "with open(r\"D:\\Ardunio_Ide\\Nhung\\Full\\vi_vocab.pkl\", \"rb\") as f:\n",
    "    vi_vocab = pickle.load(f)\n",
    "vi_vectorization = TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=sequence_length,\n",
    ")\n",
    "vi_vectorization.set_vocabulary(vi_vocab)\n",
    "eng_vectorization = TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=sequence_length + 1,\n",
    "    standardize=custom_standardization,\n",
    ")\n",
    "eng_vectorization.set_vocabulary(eng_vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"transformer\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"transformer\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">âââââââââââââââââââââââ³ââââââââââââââââââââ³âââââââââââââ³ââââââââââââââââââââ\n",
       "â<span style=\"font-weight: bold\"> Layer (type)        </span>â<span style=\"font-weight: bold\"> Output Shape      </span>â<span style=\"font-weight: bold\">    Param # </span>â<span style=\"font-weight: bold\"> Connected to      </span>â\n",
       "â¡âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ©\n",
       "â encoder_inputs      â (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      â          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â -                 â\n",
       "â (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        â                   â            â                   â\n",
       "âââââââââââââââââââââââ¼ââââââââââââââââââââ¼âââââââââââââ¼ââââââââââââââââââââ¤\n",
       "â decoder_inputs      â (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      â          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â -                 â\n",
       "â (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        â                   â            â                   â\n",
       "âââââââââââââââââââââââ¼ââââââââââââââââââââ¼âââââââââââââ¼ââââââââââââââââââââ¤\n",
       "â positional_embeddiâ¦ â (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) â <span style=\"color: #00af00; text-decoration-color: #00af00\">12,807,680</span> â encoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>â¦ â\n",
       "â (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PositionalEmbeddiâ¦</span> â                   â            â                   â\n",
       "âââââââââââââââââââââââ¼ââââââââââââââââââââ¼âââââââââââââ¼ââââââââââââââââââââ¤\n",
       "â not_equal           â (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      â          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â encoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>â¦ â\n",
       "â (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)          â                   â            â                   â\n",
       "âââââââââââââââââââââââ¼ââââââââââââââââââââ¼âââââââââââââ¼ââââââââââââââââââââ¤\n",
       "â positional_embeddiâ¦ â (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) â <span style=\"color: #00af00; text-decoration-color: #00af00\">12,807,680</span> â decoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>â¦ â\n",
       "â (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PositionalEmbeddiâ¦</span> â                   â            â                   â\n",
       "âââââââââââââââââââââââ¼ââââââââââââââââââââ¼âââââââââââââ¼ââââââââââââââââââââ¤\n",
       "â transformer_encoder â (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) â  <span style=\"color: #00af00; text-decoration-color: #00af00\">3,155,456</span> â positional_embedâ¦ â\n",
       "â (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncodeâ¦</span> â                   â            â not_equal[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   â\n",
       "âââââââââââââââââââââââ¼ââââââââââââââââââââ¼âââââââââââââ¼ââââââââââââââââââââ¤\n",
       "â not_equal_1         â (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      â          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â decoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>â¦ â\n",
       "â (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)          â                   â            â                   â\n",
       "âââââââââââââââââââââââ¼ââââââââââââââââââââ¼âââââââââââââ¼ââââââââââââââââââââ¤\n",
       "â transformer_decoder â (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) â  <span style=\"color: #00af00; text-decoration-color: #00af00\">5,259,520</span> â positional_embedâ¦ â\n",
       "â (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerDecodeâ¦</span> â                   â            â transformer_encoâ¦ â\n",
       "â                     â                   â            â not_equal_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>â¦ â\n",
       "â                     â                   â            â not_equal[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   â\n",
       "âââââââââââââââââââââââ¼ââââââââââââââââââââ¼âââââââââââââ¼ââââââââââââââââââââ¤\n",
       "â dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) â (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) â          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â transformer_decoâ¦ â\n",
       "âââââââââââââââââââââââ¼ââââââââââââââââââââ¼âââââââââââââ¼ââââââââââââââââââââ¤\n",
       "â dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     â (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      â <span style=\"color: #00af00; text-decoration-color: #00af00\">12,850,000</span> â dropout_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   â\n",
       "â                     â <span style=\"color: #00af00; text-decoration-color: #00af00\">50000</span>)            â            â                   â\n",
       "âââââââââââââââââââââââ´ââââââââââââââââââââ´âââââââââââââ´ââââââââââââââââââââ\n",
       "</pre>\n"
      ],
      "text/plain": [
       "âââââââââââââââââââââââ³ââââââââââââââââââââ³âââââââââââââ³ââââââââââââââââââââ\n",
       "â\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0mâ\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0mâ\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0mâ\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0mâ\n",
       "â¡âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ©\n",
       "â encoder_inputs      â (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      â          \u001b[38;5;34m0\u001b[0m â -                 â\n",
       "â (\u001b[38;5;33mInputLayer\u001b[0m)        â                   â            â                   â\n",
       "âââââââââââââââââââââââ¼ââââââââââââââââââââ¼âââââââââââââ¼ââââââââââââââââââââ¤\n",
       "â decoder_inputs      â (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      â          \u001b[38;5;34m0\u001b[0m â -                 â\n",
       "â (\u001b[38;5;33mInputLayer\u001b[0m)        â                   â            â                   â\n",
       "âââââââââââââââââââââââ¼ââââââââââââââââââââ¼âââââââââââââ¼ââââââââââââââââââââ¤\n",
       "â positional_embeddiâ¦ â (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m) â \u001b[38;5;34m12,807,680\u001b[0m â encoder_inputs[\u001b[38;5;34m0\u001b[0mâ¦ â\n",
       "â (\u001b[38;5;33mPositionalEmbeddiâ¦\u001b[0m â                   â            â                   â\n",
       "âââââââââââââââââââââââ¼ââââââââââââââââââââ¼âââââââââââââ¼ââââââââââââââââââââ¤\n",
       "â not_equal           â (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      â          \u001b[38;5;34m0\u001b[0m â encoder_inputs[\u001b[38;5;34m0\u001b[0mâ¦ â\n",
       "â (\u001b[38;5;33mNotEqual\u001b[0m)          â                   â            â                   â\n",
       "âââââââââââââââââââââââ¼ââââââââââââââââââââ¼âââââââââââââ¼ââââââââââââââââââââ¤\n",
       "â positional_embeddiâ¦ â (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m) â \u001b[38;5;34m12,807,680\u001b[0m â decoder_inputs[\u001b[38;5;34m0\u001b[0mâ¦ â\n",
       "â (\u001b[38;5;33mPositionalEmbeddiâ¦\u001b[0m â                   â            â                   â\n",
       "âââââââââââââââââââââââ¼ââââââââââââââââââââ¼âââââââââââââ¼ââââââââââââââââââââ¤\n",
       "â transformer_encoder â (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m) â  \u001b[38;5;34m3,155,456\u001b[0m â positional_embedâ¦ â\n",
       "â (\u001b[38;5;33mTransformerEncodeâ¦\u001b[0m â                   â            â not_equal[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   â\n",
       "âââââââââââââââââââââââ¼ââââââââââââââââââââ¼âââââââââââââ¼ââââââââââââââââââââ¤\n",
       "â not_equal_1         â (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      â          \u001b[38;5;34m0\u001b[0m â decoder_inputs[\u001b[38;5;34m0\u001b[0mâ¦ â\n",
       "â (\u001b[38;5;33mNotEqual\u001b[0m)          â                   â            â                   â\n",
       "âââââââââââââââââââââââ¼ââââââââââââââââââââ¼âââââââââââââ¼ââââââââââââââââââââ¤\n",
       "â transformer_decoder â (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m) â  \u001b[38;5;34m5,259,520\u001b[0m â positional_embedâ¦ â\n",
       "â (\u001b[38;5;33mTransformerDecodeâ¦\u001b[0m â                   â            â transformer_encoâ¦ â\n",
       "â                     â                   â            â not_equal_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0mâ¦ â\n",
       "â                     â                   â            â not_equal[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   â\n",
       "âââââââââââââââââââââââ¼ââââââââââââââââââââ¼âââââââââââââ¼ââââââââââââââââââââ¤\n",
       "â dropout_3 (\u001b[38;5;33mDropout\u001b[0m) â (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m) â          \u001b[38;5;34m0\u001b[0m â transformer_decoâ¦ â\n",
       "âââââââââââââââââââââââ¼ââââââââââââââââââââ¼âââââââââââââ¼ââââââââââââââââââââ¤\n",
       "â dense_4 (\u001b[38;5;33mDense\u001b[0m)     â (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,      â \u001b[38;5;34m12,850,000\u001b[0m â dropout_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   â\n",
       "â                     â \u001b[38;5;34m50000\u001b[0m)            â            â                   â\n",
       "âââââââââââââââââââââââ´ââââââââââââââââââââ´âââââââââââââ´ââââââââââââââââââââ\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">93,760,674</span> (357.67 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m93,760,674\u001b[0m (357.67 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">46,880,336</span> (178.83 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m46,880,336\u001b[0m (178.83 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">46,880,338</span> (178.83 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m46,880,338\u001b[0m (178.83 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loaded_transformer.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_vocab = eng_vectorization.get_vocabulary()\n",
    "eng_index_lookup = dict(zip(range(len(eng_vocab)), eng_vocab))\n",
    "max_decoded_sentence_length = 30\n",
    "\n",
    "\n",
    "def decode_sequence(input_sentence):\n",
    "    tokenized_input_sentence = vi_vectorization([input_sentence])\n",
    "    decoded_sentence = \"[start]\"\n",
    "    for i in range(max_decoded_sentence_length):\n",
    "        tokenized_target_sentence = eng_vectorization([decoded_sentence])[:, :-1]\n",
    "        predictions = loaded_transformer(\n",
    "            {\n",
    "                \"encoder_inputs\": tokenized_input_sentence,\n",
    "                \"decoder_inputs\": tokenized_target_sentence,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # ops.argmax(predictions[0, i, :]) is not a concrete value for jax here\n",
    "        sampled_token_index = ops.convert_to_numpy(\n",
    "            ops.argmax(predictions[0, i, :])\n",
    "        ).item(0)\n",
    "        sampled_token = eng_index_lookup[sampled_token_index]\n",
    "        decoded_sentence += \" \" + sampled_token\n",
    "\n",
    "        if sampled_token == \"[end]\":\n",
    "            break\n",
    "    return decoded_sentence\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'load the sound in your memory'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response  = decode_sequence(\"Táº£i Ã¢m thanh vÃ o bá» nhá» \")\n",
    "\n",
    "response = response.replace(\"[start] \",\"\")\n",
    "response = response.replace(\" [end]\",\"\")\n",
    "response= response.replace(\" m \", \"'m \")\n",
    "response= response.replace(\" s \", \"'s \")\n",
    "response=response.replace(\" d \", \"'d \")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================start===========\n",
      "======================recording===========================\n",
      "Báº¡n vá»«a nÃ³i: Xin chÃ o cÃ¡c báº¡n Xin chÃ o cÃ¡c báº¡n Xin chÃ o cÃ¡c báº¡n\n",
      "hi guys come on guys\n"
     ]
    }
   ],
   "source": [
    "import speech_recognition as sr\n",
    "from gtts import gTTS\n",
    "import io\n",
    "\n",
    "import pygame\n",
    "\n",
    "\n",
    "def text2speech(text):\n",
    "    # Táº¡o Ã¢m thanh báº±ng gTTS\n",
    "    tts = gTTS(text, lang='en')\n",
    "\n",
    "    # Táº£i Ã¢m thanh vÃ o bá» nhá» (BytesIO)\n",
    "    mp3_fp = io.BytesIO()\n",
    "    tts.write_to_fp(mp3_fp)\n",
    "    mp3_fp.seek(0)\n",
    "\n",
    "    # PhÃ¡t Ã¢m thanh vá»i pygame\n",
    "    pygame.mixer.init()\n",
    "    pygame.mixer.music.load(mp3_fp, 'mp3')\n",
    "    pygame.mixer.music.play()\n",
    "    # Chá» Äáº¿n khi Ã¢m thanh phÃ¡t xong\n",
    "    while pygame.mixer.music.get_busy(): \n",
    "        pygame.time.Clock().tick(10)  \n",
    "\n",
    "if __name__ == '__main__':\n",
    "   \n",
    "\n",
    "    recognizer = sr.Recognizer()\n",
    "    # mic = sr.Microphone()\n",
    "\n",
    "    print('=====================start===========')\n",
    "    # while(True):\n",
    "    print('======================recording===========================')\n",
    "    # with mic as source:\n",
    "        # recognizer.adjust_for_ambient_noise(source, duration=1)\n",
    "        # audio = recognizer.listen(source,timeout=None, phrase_time_limit=5)\n",
    "    # Chuyá»n Äá»i Ã¢m thanh thÃ nh vÄn báº£n\n",
    "    with sr.AudioFile(\"output.wav\") as source:\n",
    "        audio_data = recognizer.record(source)  # Äá»c toÃ n bá» file WAV\n",
    "\n",
    "    try:\n",
    "        # Chuyá»n Ã¢m thanh thÃ nh vÄn báº£n (há» trá»£ tiáº¿ng Viá»t)\n",
    "        text = recognizer.recognize_google(audio_data, language='vi-VN')\n",
    "        # if(text.lower().find('stop')>=0 or text.lower().find('dá»«ng')>=0): break\n",
    "        print(\"Báº¡n vá»«a nÃ³i:\", text)\n",
    "        \n",
    "        response = decode_sequence(text)\n",
    "        response = response.replace(\"[start] \",\"\")\n",
    "        response = response.replace(\" [end]\",\"\")\n",
    "        response = response.replace(\"[start]\",\"\")\n",
    "        response = response.replace(\"[end]\",\"\")\n",
    "        response= response.replace(\" m \", \"'m \")\n",
    "        response= response.replace(\" s \", \"'s \")\n",
    "        response=response.replace(\" d \", \"'d \")\n",
    "        response=response.replace(\" re \", \"'re \")\n",
    "        # if response==\"\": continue\n",
    "        print(response)\n",
    "        text2speech(response)\n",
    "    except sr.UnknownValueError:\n",
    "        print(\"KhÃ´ng nháº­n dáº¡ng ÄÆ°á»£c giá»ng nÃ³i.\")\n",
    "    except sr.RequestError as e:\n",
    "        print(f\"Lá»i káº¿t ná»i: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
